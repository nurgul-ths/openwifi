"""
script_process_side_ch_files.py

This script processes side channel data files generated by ch_hopping_ctl.c and dac_intf_canc_ctl.c
in the OpenWiFi project. It reads and processes multi-part capture files created by the OpenWiFi boards,
extracts experiment parameters, processes IQ data, and updates log files.

The script handles files that are split into multiple parts (fpart0, fpart1, etc.) to accommodate
large datasets that exceed file size limits.

Usage:
1. Set the PATH_DATA variable to point to the directory containing capture files.
2. Run the script: python script_process_side_ch_files.py

The script will process all files with names containing '_capture_side_ch_side_info_' in the specified directory and its subdirectories.

RUNNING: Must be run from the openwifi directory!

Additionally, the script allows specifying the number of CPU cores to use for multiprocessing.
- `--num-cores 1`: Disables multiprocessing and runs serially (useful for debugging).
- `--num-cores -1`: Uses all available CPU cores (default behavior).
- `--num-cores N`: Uses `N` CPU cores, where `N` is a positive integer greater than 1.
"""

import json
import os
from glob import glob
from owpy.files.files import gen_files, close_files, update_log_file
from owpy.capture.data_parsers import process_and_save_iq, get_num_dma_symbol_per_trans
from owpy.logging.logging import create_logger
from owpy.capture.file_parser import read_data_from_file
from types import SimpleNamespace
from multiprocessing import Pool, cpu_count
import argparse

#===============================================================================
# Constants
#===============================================================================

# PATH_DATA = "/home/andreas/gitlab_wisense/devices/openwifi_boards/data/raw/channel_hopping_movement/zed_fmcs2/"
# PATH_DATA = "/home/andreas/gitlab_wisense/devices/openwifi_boards/data/raw/channel_hopping_movement/zed_fmcs2/rx_iq0_iq1/air/"
PATH_DATA = "/home/andreas/gitlab_wisense/devices/openwifi_boards/data/raw/channel_hopping_movement/zed_fmcs2/rx_iq0_iq1/air/"

#===============================================================================
# Functions
#===============================================================================


def check_and_change_directory():
  """
  Checks if the script is run from within the 'openwifi_boards' directory and changes to it.

  This function verifies that the script is being executed from a directory within
  'openwifi_boards'. If it is, it changes the working directory to 'openwifi_boards'.
  If not, it raises a RuntimeError.

  Raises:
    RuntimeError: If the script is not run from within the 'openwifi_boards' directory.

  Returns:
    None
  """
  current_dir = os.getcwd()
  parent_dir  = os.path.dirname(current_dir)

  if os.path.basename(parent_dir) != "openwifi_boards":
    raise RuntimeError("Script must be run from a directory within 'openwifi_boards'")

  os.chdir(parent_dir)
  print(f"Working directory changed to: {os.getcwd()}")


def load_params_from_log(log_file_path):
  """
  Load experiment parameters from a log file.

  Args:
    log_file_path (str): Path to the log file.

  Returns:
    SimpleNamespace: Namespace containing experiment parameters.
  """
  print(f"Loading parameters from {log_file_path}")
  with open(log_file_path, 'r', encoding='latin-1', errors='replace') as log_file:
    params = json.load(log_file)
  return SimpleNamespace(**params)


def get_capture_file_paths(folder_path):
  """
  Get the paths of all side channel data files in a folder.

  Args:
    folder_path (str): Path to the folder containing capture files.

  Returns:
    list: List of paths to capture files.
  """
  capture_file_paths = []
  for root, _, files in os.walk(folder_path):
    for file in files:
      if "_capture_side_ch_side_info_" in file and file.endswith(".txt"):
        capture_file_paths.append(os.path.join(root, file))
  return capture_file_paths


def extract_fpart_number(filename):
  """
  Extract the numerical part from the '_fpart' suffix in the filename.

  This function is used to sort multi-part files in the correct order.

  Args:
    filename (str): Name of the file.

  Returns:
    int: Extracted part number or -1 if not found.
  """
  parts = filename.split("_fpart")
  if len(parts) > 1:
    try:
      return int(parts[1].split('.')[0])
    except ValueError:
      return -1
  return -1


def process_capture_files(capture_file_paths, num_cores=-1):
  """
  Process the capture files generated by ch_hopping_ctl.c and dac_intf_canc_ctl.c.

  This function handles multi-part capture files, extracts parameters from associated
  log files, and processes the IQ data. It performs the following steps:
  1. Groups files by their base name (without _fpartX suffix)
  2. Creates tasks for parallel processing or runs serially based on num_cores
  3. Processes each group of files accordingly
  4. Saves processed data and updates log files

  Args:
    capture_file_paths (list): List of paths to capture files.
    num_cores (int): Number of CPU cores to use.
                     -1: Use all available cores.
                      1: Disable multiprocessing (run serially).
                      N: Use N CPU cores.

  Returns:
    None
  """
  logger = create_logger("process_side_ch_files")
  logger.info("Processing side channel data files")

  # Group files by their base name (without _fpartX suffix)
  base_files = set(f.split("_fpart")[0] for f in capture_file_paths)

  # Create tasks for parallel processing
  tasks = [(base_file, capture_file_paths) for base_file in base_files]

  if num_cores == 1:
    # Disable multiprocessing and run serially
    print("Multiprocessing disabled. Running in serial mode.")
    for task in tasks:
      process_base_file(*task)
  else:
    # Determine the number of processes
    if num_cores == -1:
      num_processes = cpu_count()
    elif num_cores > 1:
      num_processes = min(num_cores, cpu_count())
    else:
      raise ValueError("Invalid value for num_cores. Use -1 for all cores or a positive integer.")

    print(f"Using {num_processes} CPU core(s) for multiprocessing.")

    with Pool(processes=num_processes) as pool:
      pool.starmap(process_base_file, tasks)


def process_base_file(base_file, capture_file_paths):
  """
  Process a single base file and its associated part files.

  This function handles the processing of a group of files sharing the same base name.
  It loads parameters, processes each part file, and updates logs accordingly.

  Args:
    base_file (str): The base name of the file group to process.
    capture_file_paths (list): List of all capture file paths.

  Returns:
    None
  """

  logger = create_logger(f"process_side_ch_files_{os.getpid()}")
  logger.info(f"Processing base file: {base_file}")

  log_file_path = f"{base_file.split('_capture_side_ch_side_info')[0]}_openwifi_log.txt"
  if not os.path.exists(log_file_path):
    logger.error(f"Log file does not exist for {base_file}")
    return

  params = load_params_from_log(log_file_path)
  iq_num_dma_symbol_per_trans, _ = get_num_dma_symbol_per_trans(params)
  fd_dict                        = gen_files(params)
  total_frames                   = 0

  # Sort part files to ensure correct processing order
  part_files = sorted((f for f in capture_file_paths if f.startswith(base_file)), key=extract_fpart_number)
  processed_file_path = f"{base_file}.processed"

  if not os.path.exists(processed_file_path):
    for part in part_files:
      logger.info(f"Processing {part}")
      data_list = read_data_from_file(part)

      for data in data_list:
        n_frames, _ = process_and_save_iq(data, fd_dict, iq_num_dma_symbol_per_trans, params)
        total_frames += n_frames

    open(processed_file_path, 'a').close()  # Mark as processed
  else:
    logger.info(f"Skipping {base_file} as it has already been processed.")

  if not params.save_data:
    update_log_file(params, {"total_frames": total_frames})
    close_files(fd_dict)

  logger.info(f"Completed processing base file: {base_file}. Total frames: {total_frames}")


#===============================================================================
# Main
#===============================================================================

def main():
  """
  Main function to initiate the processing of capture files.

  This function first checks and changes the working directory to ensure
  the script is run from the correct location. It then proceeds to get
  the capture file paths and process them.

  Additionally, it allows specifying the number of CPU cores to use for multiprocessing.
  - `--num-cores 1`: Disables multiprocessing and runs serially (useful for debugging).
  - `--num-cores -1`: Uses all available CPU cores (default behavior).
  - `--num-cores N`: Uses `N` CPU cores, where `N` is a positive integer greater than 1.

  If the directory check fails, the function prints an error message and returns.
  """
  parser = argparse.ArgumentParser(description="Process side channel capture files with optional multiprocessing.")
  parser.add_argument("--num-cores", type=int, default=-1, help="Number of CPU cores to use. Use -1 for all available cores, 1 to disable multiprocessing (useful for debugging), or specify a positive integer for N cores.")
  args = parser.parse_args()

  try:
    check_and_change_directory()
    capture_file_paths = get_capture_file_paths(PATH_DATA)
    print(f"Found {len(capture_file_paths)} capture files to process.")
    process_capture_files(capture_file_paths, num_cores=args.num_cores)
  except RuntimeError as e:
    print(f"Error: {e}")
  except ValueError as ve:
    print(f"Argument Error: {ve}")


if __name__ == "__main__":
  main()





































# def process_base_file(base_file):

#   for base_file in base_files:
#     logger.info(f"Processing base file: {base_file}")

#     # Find and load the associated log file
#     log_file_path = f"{base_file.split('_capture_side_ch_side_info')[0]}_openwifi_log.txt"
#     if not os.path.exists(log_file_path):
#       logger.error(f"Log file does not exist for {base_file}")
#       continue

#     params = load_params_from_log(log_file_path)
#     iq_num_dma_symbol_per_trans, _ = get_num_dma_symbol_per_trans(params)
#     fd_dict                        = gen_files(params)
#     total_frames                   = 0

#     # Sort part files to ensure correct processing order
#     part_files = sorted((f for f in capture_file_paths if f.startswith(base_file)), key=extract_fpart_number)
#     processed_file_path = f"{base_file}.processed"

#     if not os.path.exists(processed_file_path):
#       for part in part_files:
#         logger.info(f"Processing {part}")
#         data_list = read_data_from_file(part)

#         for data in data_list:
#           n_frames, _ = process_and_save_iq(data, fd_dict, iq_num_dma_symbol_per_trans, params)
#           total_frames += n_frames

#       open(processed_file_path, 'a').close()  # Mark as processed
#     else:
#       logger.info(f"Skipping {base_file} as it has already been processed.")

#     if not params.save_data:
#       update_log_file(params, {"total_frames": total_frames})
#       close_files(fd_dict)

#     logger.info(f"Completed processing base file: {base_file}. Total frames: {total_frames}")











# """


# """

# import json
# import os
# from glob import glob
# from owpy.files.files import gen_files, close_files, update_log_file
# from owpy.capture.data_parsers import process_and_save_iq
# from owpy.logging.logging import create_logger
# from owpy.capture.data_parsers import get_num_dma_symbol_per_trans
# from owpy.capture.file_parser import read_data_from_file
# from types import SimpleNamespace

# def load_params_from_log(log_file_path):
#   """Load experiment parameters from a log file."""

#   print(f"Loading parameters from {log_file_path}")
#   with open(log_file_path, 'r', encoding='latin-1', errors='replace') as log_file:
#     params = json.load(log_file)

#   params = SimpleNamespace(**params)
#   return params

# def get_capture_file_paths(folder_path):
#   """Get the paths of all side channel data files in a folder. All side ch files have names added to them matching _capture_side_ch_side_info_*.txt pattern"""

#   capture_file_paths = []
#   for root, dirs, files in os.walk(folder_path):
#     for file in files:
#       if "_capture_side_ch_side_info_" in file and file.endswith(".txt"):
#         capture_file_paths.append(os.path.join(root, file))

#   return capture_file_paths


# def extract_fpart_number(filename):
#   """ Extract the numerical part from the '_fpart' suffix in the filename """
#   parts = filename.split("_fpart")
#   if len(parts) > 1:
#     try:
#       return int(parts[1].split('.')[0])
#     except ValueError:
#       return -1  # Return an invalid number if conversion fails
#   return -1


# def process_capture_files(capture_file_paths):
#   """
#   Process the capture files, note that we get a set of files as follows with parts

#   label-path0_0cm_path1_600cm_2024-05-15_22-11-40_capture_side_ch_side_info_m1_e1_t240_v0_f2377_c8_z15_b1000_p0_q40000_20240504_043344_fpart0.txt
#   label-path0_0cm_path1_600cm_2024-05-15_22-11-40_capture_side_ch_side_info_m1_e1_t240_v0_f2377_c8_z15_b1000_p0_q40000_20240504_043344_fpart1.txt
#   label-path0_0cm_path1_600cm_2024-05-15_22-11-40_capture_side_ch_side_info_m1_e1_t240_v0_f2377_c8_z15_b1000_p0_q40000_20240504_043344_fpart2.txt
#   label-path0_0cm_path1_600cm_2024-05-15_22-11-40_capture_side_ch_side_info_m1_e1_t240_v0_f2377_c8_z15_b1000_p0_q40000_20240504_043344_fpart3.txt
#   label-path0_0cm_path1_600cm_2024-05-15_22-11-40_capture_side_ch_side_info_m1_e1_t240_v0_f2377_c8_z15_b1000_p0_q40000_20240504_043344_fpart4.txt

#   Then, we should first sort the files and then process them in order. We need to get the base name which is given as the file name
#   without the .txt part and the _fpart* part at the end, however, note that there can be _p in the name

#   """

#   logger = create_logger("process_side_ch_files")
#   logger.info("Processing side channel data files")

#   base_files = set(f.split("_fpart")[0] for f in capture_file_paths)

#   for base_file in base_files:
#     logger.info(f"Processing base file: {base_file}")

#     log_file_path = f"{base_file.split('_capture_side_ch_side_info')[0]}_openwifi_log.txt"

#     if os.path.exists(log_file_path):
#       params = load_params_from_log(log_file_path)
#     else:
#       logger.error(f"Log file does not exist for {base_file}")
#       continue

#     iq_num_dma_symbol_per_trans, csi_num_dma_symbol_per_trans = get_num_dma_symbol_per_trans(params)
#     fd_dict = gen_files(params)
#     total_frames = 0

#     part_files = sorted((f for f in capture_file_paths if f.startswith(base_file)), key=extract_fpart_number)
#     processed_file_path = f"{base_file}.processed"

#     if not os.path.exists(processed_file_path):
#       for part in part_files:
#         logger.info(f"Processing {part}")
#         data_list = read_data_from_file(part)

#         for data in data_list:
#           n_frames, data_dict = process_and_save_iq(data, fd_dict, iq_num_dma_symbol_per_trans, params)
#           total_frames += n_frames

#       # Mark the base file as processed
#       open(processed_file_path, 'a').close()
#     else:
#       logger.info(f"Skipping {base_file} as it has already been processed.")

#     if not params.save_data:
#       update_log_file(params, {"total_frames": total_frames})
#       close_files(fd_dict)

#     logger.info(f"Completed processing base file: {base_file}. Total frames: {total_frames}")



# if __name__ == "__main__":
#   # folder_path = "/home/andreas/gitlab_wisense/devices/openwifi_boards/data/raw"
#   folder_path = "/home/andreas/gitlab_wisense/devices/openwifi_boards/data/raw/channel_hopping_movement/zed_fmcs2/rx_iq0_iq1/air/channel_fixed_calibration_a1/2024-10-12/"
#   capture_file_paths = get_capture_file_paths(folder_path)
#   print(capture_file_paths)
#   process_capture_files(capture_file_paths)












# """
# script_process_side_ch_files.py

# This script processes side channel data files generated by ch_hopping_ctl.c and dac_intf_canc_ctl.c
# in the OpenWiFi project. It reads and processes multi-part capture files created by the OpenWiFi boards,
# extracts experiment parameters, processes IQ data, and updates log files.

# The script handles files that are split into multiple parts (fpart0, fpart1, etc.) to accommodate
# large datasets that exceed file size limits.

# Usage:
# 1. Set the PATH_DATA variable to point to the directory containing capture files.
# 2. Run the script: python script_process_side_ch_files.py

# The script will process all files with names containing '_capture_side_ch_side_info_' in the specified directory and its subdirectories.
# """

# import json
# import os
# from glob import glob
# from owpy.files.files import gen_files, close_files, update_log_file
# from owpy.capture.data_parsers import process_and_save_iq, get_num_dma_symbol_per_trans
# from owpy.logging.logging import create_logger
# from owpy.capture.file_parser import read_data_from_file
# from types import SimpleNamespace

# #===============================================================================
# # Constants
# #===============================================================================

# # PATH_DATA = "/home/andreas/gitlab_wisense/devices/openwifi_boards/data/raw/channel_hopping_movement/zed_fmcs2/"
# PATH_DATA = "/home/andreas/gitlab_wisense/devices/openwifi_boards/data/raw/channel_hopping_movement/zed_fmcs2/rx_iq0_iq1/air/channel_fixed_calibration_a1/2024-10-12/label-calibration_2024-10-12_06-05-24"

# #===============================================================================
# # Functions
# #===============================================================================

# def load_params_from_log(log_file_path):
#   """
#   Load experiment parameters from a log file.

#   Args:
#     log_file_path (str): Path to the log file.

#   Returns:
#     SimpleNamespace: Namespace containing experiment parameters.
#   """
#   print(f"Loading parameters from {log_file_path}")
#   with open(log_file_path, 'r', encoding='latin-1', errors='replace') as log_file:
#     params = json.load(log_file)
#   return SimpleNamespace(**params)

# def get_capture_file_paths(folder_path):
#   """
#   Get the paths of all side channel data files in a folder.

#   Args:
#     folder_path (str): Path to the folder containing capture files.

#   Returns:
#     list: List of paths to capture files.
#   """
#   capture_file_paths = []
#   for root, _, files in os.walk(folder_path):
#     for file in files:
#       if "_capture_side_ch_side_info_" in file and file.endswith(".txt"):
#         capture_file_paths.append(os.path.join(root, file))
#   return capture_file_paths

# def extract_fpart_number(filename):
#   """
#   Extract the numerical part from the '_fpart' suffix in the filename.

#   This function is used to sort multi-part files in the correct order.

#   Args:
#     filename (str): Name of the file.

#   Returns:
#     int: Extracted part number or -1 if not found.
#   """
#   parts = filename.split("_fpart")
#   if len(parts) > 1:
#     try:
#       return int(parts[1].split('.')[0])
#     except ValueError:
#       return -1
#   return -1

# def process_capture_files(capture_file_paths):
#   """
#   Process the capture files generated by ch_hopping_ctl.c and dac_intf_canc_ctl.c.

#   This function handles multi-part capture files, extracts parameters from associated
#   log files, and processes the IQ data. It performs the following steps:
#   1. Groups files by their base name (without _fpartX suffix)
#   2. Loads parameters from associated log files
#   3. Processes each group of files in order
#   4. Saves processed data and updates log files

#   Args:
#     capture_file_paths (list): List of paths to capture files.
#   """
#   logger = create_logger("process_side_ch_files")
#   logger.info("Processing side channel data files")

#   # Group files by their base name (without _fpartX suffix)
#   base_files = set(f.split("_fpart")[0] for f in capture_file_paths)

#   for base_file in base_files:
#     logger.info(f"Processing base file: {base_file}")

#     # Find and load the associated log file
#     log_file_path = f"{base_file.split('_capture_side_ch_side_info')[0]}_openwifi_log.txt"
#     if not os.path.exists(log_file_path):
#       logger.error(f"Log file does not exist for {base_file}")
#       continue

#     params = load_params_from_log(log_file_path)
#     iq_num_dma_symbol_per_trans, _ = get_num_dma_symbol_per_trans(params)
#     fd_dict                        = gen_files(params)
#     total_frames                   = 0

#     # Sort part files to ensure correct processing order
#     part_files        = sorted((f for f in capture_file_paths if f.startswith(base_file)), key=extract_fpart_number)
#     processed_file_path = f"{base_file}.processed"

#     if not os.path.exists(processed_file_path):
#       for part in part_files:
#         logger.info(f"Processing {part}")
#         data_list = read_data_from_file(part)

#         for data in data_list:
#           n_frames, _ = process_and_save_iq(data, fd_dict, iq_num_dma_symbol_per_trans, params)
#           total_frames += n_frames

#       open(processed_file_path, 'a').close()  # Mark as processed
#     else:
#       logger.info(f"Skipping {base_file} as it has already been processed.")

#     if not params.save_data:
#       update_log_file(params, {"total_frames": total_frames})
#       close_files(fd_dict)

#     logger.info(f"Completed processing base file: {base_file}. Total frames: {total_frames}")

# #===============================================================================
# # Main
# #===============================================================================

# def main():
#   """
#   Main function to initiate the processing of capture files.
#   """
#   capture_file_paths = get_capture_file_paths(PATH_DATA)
#   print(capture_file_paths)
#   process_capture_files(capture_file_paths)

# if __name__ == "__main__":
#   main()
